{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leitura do texto central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"data/HN_posts_year_to_Sep_26_2016.csv\"\n",
    "language = 'english'\n",
    "df = pd.read_csv(inputFile)\n",
    "corpus = df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"data/BrasCubas-Assis.txt\"\n",
    "language = 'portuguese'\n",
    "with open(inputFile) as f:\n",
    "    corpus = f.readlines()\n",
    "    corpus = [paragraph.rstrip() for paragraph in corpus if paragraph != '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pré-processamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctranslation = str.maketrans(dict.fromkeys(punctuation))\n",
    "setStopwords = set(stopwords.words(language))\n",
    "\n",
    "# Realiza a tokenização e tratamento dos parágrafos do texto\n",
    "def tokenize(corpus: str) -> list:\n",
    "    corpusTokenized = []\n",
    "    for paragraph in corpus:\n",
    "        paragraph = paragraph.lower()                                       # Tratamento de case-sensitive\n",
    "        paragraph = paragraph.encode('utf8', 'ignore').decode()             # Eliminação de caracteres fora de UTF-8\n",
    "        paragraph = paragraph.translate(punctranslation)                    # Eliminação de pontuações\n",
    "        corpusTokenized.append([token for token in paragraph.split()        # Eliminação de stopwords\n",
    "                                if token not in setStopwords])\n",
    "    return corpusTokenized\n",
    "    \n",
    "corpusTokenized = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Contagem de unigramas do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "for paragraph in corpusTokenized:\n",
    "    for token in paragraph:\n",
    "        unigrams[token] += 1\n",
    "\n",
    "# Mapeamento de acesso (Token <-> Índice representativo)\n",
    "token2index = {token: index for index, token in enumerate(unigrams.keys())}\n",
    "index2token = {index: token for token, index in token2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contagem de bigramas do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('show', 'hn', 10263),\n",
       " ('hn', 'show', 10263),\n",
       " ('ask', 'hn', 9255),\n",
       " ('hn', 'ask', 9255),\n",
       " ('open', 'source', 2036),\n",
       " ('source', 'open', 2036),\n",
       " ('machine', 'learning', 1211),\n",
       " ('learning', 'machine', 1211),\n",
       " ('silicon', 'valley', 1060),\n",
       " ('valley', 'silicon', 1060)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contador de bigramas, considerando uma determinada janela de contexto (nesse caso, 2\n",
    "# palavras antes e duas palavras depois)\n",
    "skipgrams = Counter()\n",
    "gap = 3\n",
    "\n",
    "for paragraph in corpusTokenized:\n",
    "    tokens = [token2index[tok] for tok in paragraph]\n",
    "    \n",
    "    # Para cada palavra no parágrafo, realiza a análise dos contextos da vizinhança\n",
    "    for indexWord, word in enumerate(paragraph):\n",
    "        indexContextMin = max(0, indexWord - gap)\n",
    "        indexContextMax = min(len(paragraph)-1, indexWord + gap)\n",
    "\n",
    "        # Para cada contexto da vizinhança, crie um bigrama com a palavra central\n",
    "        indexContexts = [index for index in range(indexContextMin, indexContextMax + 1) if index != indexWord]\n",
    "        for indexContext in indexContexts:\n",
    "            skipgram = (tokens[indexWord], tokens[indexContext])\n",
    "            skipgrams[skipgram] += 1\n",
    "\n",
    "# Exibição dos 10 bigramas mais comuns presentes no texto, considerando a janela de contexto estipulada\n",
    "mostCommon = [(index2token[skipgram[0][0]], index2token[skipgram[0][1]], skipgram[1]) \n",
    "               for skipgram in skipgrams.most_common(10)]\n",
    "mostCommon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Criação da matrix de frequência termo-a-termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99366x99366 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3792872 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapeamento das entradas da matriz esparça de frequência entre os bigramas do texto\n",
    "rowsMatrix = []\n",
    "columnsMatrix = []\n",
    "dataMatrix = []\n",
    "\n",
    "for (token1, token2), skipgramCount in skipgrams.items():\n",
    "    rowsMatrix.append(token1)\n",
    "    columnsMatrix.append(token2)\n",
    "    dataMatrix.append(skipgramCount)\n",
    "\n",
    "wwMatrix = sparse.csr_matrix((dataMatrix, (rowsMatrix, columnsMatrix)))\n",
    "wwMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Criação da matriz PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99366x99366 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3792872 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número total de bigramas presente na matriz de frequência\n",
    "numSkipgrams = wwMatrix.sum()\n",
    "\n",
    "# Mapeamento das entradas da matriz PPMI\n",
    "rowsIndex = []\n",
    "columnsIndex = []\n",
    "ppmiData = []\n",
    "\n",
    "# Vetor de frequência total de cada palavra em todos os possíveis contextos\n",
    "sumWords = np.array(wwMatrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Vetor de frequência total de cada contexto para todas as possíveis palavras\n",
    "sumContexts = np.array(wwMatrix.sum(axis=1)).flatten()\n",
    "\n",
    "for (tokenWord, tokenContext), skipgramCount in skipgrams.items():\n",
    "\n",
    "    # Frequência de determinada palavra em determinado contexto\n",
    "    # [#(w,c)]\n",
    "    freqWordContext = skipgramCount\n",
    "\n",
    "    # Frequência de determinada palavra em todos os contextos possíveis\n",
    "    # [#(w)]\n",
    "    freqWord = sumContexts[tokenWord]\n",
    "    \n",
    "    # Frequência de determinado contexto para todas as palavras possíveis\n",
    "    # [#(c)]\n",
    "    freqContext = sumWords[tokenContext]\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra em determinado contexto\n",
    "    # [P(w,c)]\n",
    "    probWordContext = freqWordContext / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra individualmente\n",
    "    # [P(w)]\n",
    "    probWord = freqWord / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinado contexto individualmente\n",
    "    # [P(c)]\n",
    "    probContext = freqContext / numSkipgrams\n",
    "\n",
    "    # Cálculo PPMI (Positive Pointwise Mutual Information)\n",
    "    # [PPMI = max(0, log( P(w,c)/(P(w)P(c)) ))]\n",
    "    PPMI = max(np.log2(probWordContext / (probWord * probContext)), 0)\n",
    "\n",
    "    rowsIndex.append(tokenWord)\n",
    "    columnsIndex.append(tokenContext)\n",
    "    ppmiData.append(PPMI)\n",
    "\n",
    "ppmiMatrix = sparse.csr_matrix((ppmiData, (rowsIndex, columnsIndex)))\n",
    "ppmiMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fatoração matricial usando SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds as SVD\n",
    "\n",
    "# Dimensão proposta da matriz de valores singulares produzida pelo SVD\n",
    "# [Hiperparâmetro]\n",
    "embeddingSize = 50\n",
    "\n",
    "U, D, V = SVD(ppmiMatrix, embeddingSize)\n",
    "\n",
    "# Normalização das matrizes de vetores singulares produzidas pelo SVD\n",
    "Unorm = U / np.sqrt(np.sum(U*U, axis=1, keepdims=True))\n",
    "Vnorm = V / np.sqrt(np.sum(V*V, axis=1, keepdims=True))\n",
    "\n",
    "# ???\n",
    "wordVecs = Unorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualização de palavras similares por similaridade por cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cálculo dos 10 contextos mais similares a dada palavra utilizando a matriz de Word Embedding\n",
    "def wordsSimilarity(word, matrix, n):\n",
    "    wordIndex = token2index[word]\n",
    "\n",
    "    # Resgate do vetor representante de determinada palavra\n",
    "    if isinstance(matrix, sparse.csr_matrix):\n",
    "        wordVec = matrix.getrow(wordIndex)\n",
    "    else:\n",
    "        wordVec = matrix[wordIndex:wordIndex+1, :]\n",
    "\n",
    "    # Cálculo da similidade (similaridade de vetores por cosseno)\n",
    "    similarity = cosine_similarity(matrix, wordVec).flatten()\n",
    "    sortedIndexes = np.argsort(-similarity)\n",
    "\n",
    "    # Retorno dos n contextos mais similares a dada palavra\n",
    "    similarityContextScores = [(index2token[sortedIndex], similarity[sortedIndex]) \n",
    "                                for sortedIndex in sortedIndexes[:n+1] \n",
    "                                if index2token[sortedIndex] != word]\n",
    "\n",
    "    return similarityContextScores\n",
    "\n",
    "def wordSimilarityReport(word, matrix, n=5):\n",
    "    print(f'\\'{word}\\'\\t Frequência total: {unigrams[word]}', end='\\n\\t')\n",
    "\n",
    "    similarityContextScores = wordsSimilarity(word, matrix, n)\n",
    "    for context, similarity in similarityContextScores:\n",
    "        print(f'(\\'{context}\\', {similarity})', end='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'spotify'\t Frequência total: 271\n",
      "\t('musixmatch', 0.7946992630661831)\t('soundcloud', 0.7630237337321663)\t('music', 0.7437292140911329)\t('snapchat', 0.7258352530284516)\t('beatport', 0.7068481903821319)\t\n",
      "------------------------------------------------------------\n",
      "'learning'\t Frequência total: 3092\n",
      "\t('deep', 0.9679018143288161)\t('machine', 0.9659037272340059)\t('tensorlayer', 0.9351446662279084)\t('selfstudying', 0.934205970281644)\t('javacpp', 0.9335810170788844)\t\n",
      "------------------------------------------------------------\n",
      "'deep'\t Frequência total: 1375\n",
      "\t('learning', 0.9679018143288162)\t('advers', 0.9516899496215433)\t('tinycnn', 0.9496797213952085)\t('techniquesetc', 0.9483915875145995)\t('javacpp', 0.9478161231761266)\t\n",
      "------------------------------------------------------------\n",
      "'snapchat'\t Frequência total: 342\n",
      "\t('spotify', 0.7258352530284516)\t('instagram', 0.7016543199091057)\t('tinder', 0.6955864370596595)\t('twitter', 0.6914984740807837)\t('bitstrips', 0.6894812191440871)\t\n",
      "------------------------------------------------------------\n",
      "'facebook'\t Frequência total: 2853\n",
      "\t('twitter', 0.8259671375169364)\t('ads', 0.722539678549977)\t('whatsapp', 0.7112462442950076)\t('unmonetizable', 0.703963680658755)\t('media', 0.6978895765691213)\t\n",
      "------------------------------------------------------------\n",
      "'musk'\t Frequência total: 450\n",
      "\t('elon', 0.9869655320209831)\t('outmuscle', 0.8832473668239732)\t('musks', 0.8644334306411096)\t('talkes', 0.8431256505143561)\t('sigmar', 0.843125650514356)\t\n",
      "------------------------------------------------------------\n",
      "'linux'\t Frequência total: 1803\n",
      "\t('reiser4', 0.9227035474398567)\t('tinycore', 0.9171279622370516)\t('mklinux', 0.9107477025041933)\t('v4xx', 0.9052991859945565)\t('fwupd', 0.9038064972788681)\t\n",
      "------------------------------------------------------------\n",
      "'safety'\t Frequência total: 267\n",
      "\t('lifepaint', 0.9209032835165049)\t('fingerprintbackgroundcheck', 0.8776932086197897)\t('wisewear', 0.8518603720753378)\t('sullys', 0.8474622109887953)\t('ukfor', 0.7765669816246308)\t\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = ['spotify', 'learning', 'deep', 'snapchat', 'facebook', 'musk', 'linux', 'safety']\n",
    "for word in examples:\n",
    "    wordSimilarityReport(word, wordVecs)\n",
    "    print('\\n'+'---'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Visualizando as palavras através de um ScatterPlot interativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
