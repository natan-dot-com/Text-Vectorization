{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leitura do texto central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"data/BrasCubas-Assis.txt\"\n",
    "with open(inputFile) as f:\n",
    "    corpus = f.readlines()\n",
    "    corpus = [paragraph.rstrip() for paragraph in corpus if paragraph != '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pré-processamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctranslation = str.maketrans(dict.fromkeys(punctuation))\n",
    "setStopwords = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Realiza a tokenização e tratamento dos parágrafos do texto\n",
    "def tokenize(corpus: str) -> list:\n",
    "    corpusTokenized = []\n",
    "    for paragraph in corpus:\n",
    "        paragraph = paragraph.lower()                                       # Tratamento de case-sensitive\n",
    "        paragraph = paragraph.encode('utf8', 'ignore').decode()             # Eliminação de caracteres fora de UTF-8\n",
    "        paragraph = paragraph.translate(punctranslation)                    # Eliminação de pontuações\n",
    "        corpusTokenized.append([token for token in paragraph.split()        # Eliminação de stopwords\n",
    "                                if token not in setStopwords])\n",
    "    return corpusTokenized\n",
    "    \n",
    "corpusTokenized = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Contagem de unigramas do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "for paragraph in corpusTokenized:\n",
    "    for token in paragraph:\n",
    "        unigrams[token] += 1\n",
    "\n",
    "# Mapeamento de acesso (Token <-> Índice representativo)\n",
    "token2index = {token: index for index, token in enumerate(unigrams.keys())}\n",
    "index2token = {index: token for token, index in token2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contagem de bigramas do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neve', 'neve', 12),\n",
       " ('alguma', 'coisa', 10),\n",
       " ('coisa', 'alguma', 10),\n",
       " ('idéia', 'fixa', 9),\n",
       " ('fixa', 'idéia', 9),\n",
       " ('mesma', 'coisa', 9),\n",
       " ('coisa', 'mesma', 9),\n",
       " ('sempre', 'mesma', 8),\n",
       " ('sempre', 'coisa', 8),\n",
       " ('mesma', 'sempre', 8)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contador de bigramas, considerando uma determinada janela de contexto (nesse caso, 2\n",
    "# palavras antes e duas palavras depois)\n",
    "skipgrams = Counter()\n",
    "gap = 3\n",
    "\n",
    "for paragraph in corpusTokenized:\n",
    "    tokens = [token2index[tok] for tok in paragraph]\n",
    "    \n",
    "    # Para cada palavra no parágrafo, realiza a análise dos contextos da vizinhança\n",
    "    for indexWord, word in enumerate(paragraph):\n",
    "        indexContextMin = max(0, indexWord - gap)\n",
    "        indexContextMax = min(len(paragraph)-1, indexWord + gap)\n",
    "\n",
    "        # Para cada contexto da vizinhança, crie um bigrama com a palavra central\n",
    "        indexContexts = [index for index in range(indexContextMin, indexContextMax + 1) if index != indexWord]\n",
    "        for indexContext in indexContexts:\n",
    "            skipgram = (tokens[indexWord], tokens[indexContext])\n",
    "            skipgrams[skipgram] += 1\n",
    "\n",
    "# Exibição dos 10 bigramas mais comuns presentes no texto, considerando a janela de contexto estipulada\n",
    "mostCommon = [(index2token[skipgram[0][0]], index2token[skipgram[0][1]], skipgram[1]) \n",
    "               for skipgram in skipgrams.most_common(10)]\n",
    "mostCommon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Criação da matrix de frequência termo-a-termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3285x3285 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapeamento das entradas da matriz esparça de frequência entre os bigramas do texto\n",
    "rowsMatrix = []\n",
    "columnsMatrix = []\n",
    "dataMatrix = []\n",
    "\n",
    "for (token1, token2), skipgramCount in skipgrams.items():\n",
    "    rowsMatrix.append(token1)\n",
    "    columnsMatrix.append(token2)\n",
    "    dataMatrix.append(skipgramCount)\n",
    "\n",
    "wwMatrix = sparse.csr_matrix((dataMatrix, (rowsMatrix, columnsMatrix)))\n",
    "wwMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Criação da matriz PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3285x3285 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número total de bigramas presente na matriz de frequência\n",
    "numSkipgrams = wwMatrix.sum()\n",
    "\n",
    "# Mapeamento das entradas da matriz PPMI\n",
    "rowsIndex = []\n",
    "columnsIndex = []\n",
    "ppmiData = []\n",
    "\n",
    "# Vetor de frequência total de cada palavra em todos os possíveis contextos\n",
    "sumWords = np.array(wwMatrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Vetor de frequência total de cada contexto para todas as possíveis palavras\n",
    "sumContexts = np.array(wwMatrix.sum(axis=1)).flatten()\n",
    "\n",
    "for (tokenWord, tokenContext), skipgramCount in skipgrams.items():\n",
    "\n",
    "    # Frequência de determinada palavra em determinado contexto\n",
    "    # [#(w,c)]\n",
    "    freqWordContext = skipgramCount\n",
    "\n",
    "    # Frequência de determinada palavra em todos os contextos possíveis\n",
    "    # [#(w)]\n",
    "    freqWord = sumContexts[tokenWord]\n",
    "    \n",
    "    # Frequência de determinado contexto para todas as palavras possíveis\n",
    "    # [#(c)]\n",
    "    freqContext = sumWords[tokenContext]\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra em determinado contexto\n",
    "    # [P(w,c)]\n",
    "    probWordContext = freqWordContext / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra individualmente\n",
    "    # [P(w)]\n",
    "    probWord = freqWord / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinado contexto individualmente\n",
    "    # [P(c)]\n",
    "    probContext = freqContext / numSkipgrams\n",
    "\n",
    "    # Cálculo PPMI (Positive Pointwise Mutual Information)\n",
    "    # [PPMI = max(0, log( P(w,c)/(P(w)P(c)) ))]\n",
    "    PPMI = max(np.log2(probWordContext / (probWord * probContext)), 0)\n",
    "\n",
    "    rowsIndex.append(tokenWord)\n",
    "    columnsIndex.append(tokenContext)\n",
    "    ppmiData.append(PPMI)\n",
    "\n",
    "ppmiMatrix = sparse.csr_matrix((ppmiData, (rowsIndex, columnsIndex)))\n",
    "ppmiMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fatoração matricial usando SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds as SVD\n",
    "\n",
    "# Dimensão proposta da matriz de valores singulares produzida pelo SVD\n",
    "# [Hiperparâmetro]\n",
    "embeddingSize = 50\n",
    "\n",
    "U, D, V = SVD(ppmiMatrix, embeddingSize)\n",
    "\n",
    "# Normalização das matrizes de vetores singulares produzidas pelo SVD\n",
    "Unorm = U / np.sqrt(np.sum(U*U, axis=1, keepdims=True))\n",
    "Vnorm = V / np.sqrt(np.sum(V*V, axis=1, keepdims=True))\n",
    "\n",
    "# ???\n",
    "wordVecs = Unorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualização de palavras similares por similaridade por cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: 'defunto' \t Similaridade: 1.0000000000000002\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "\"-Anda visitando os defuntos? Disse-lhe eu. -Ora, defuntos! respondeu Virgília com um muxoxo. E depois de me apertar as mãos: -Ando a ver se ponho os vadios para a rua.\"\n",
      "\n",
      "\"Logo depois, senti-me transformado na Suma Teologica de São Tomás, impressa num volume, e encadernada em marroquim, com fechos de prata e estampas; idéia esta que me deu ao corpo a mais completa imobilidade; e ainda agora me lembra que, sendo as minhas mãos os fechos do livro, e cruzando-as eu sobre o ventre, alguém as descruzava (Virgília decerto), porque a atitude lhe dava a imagem de um defunto.\"\n",
      "\n",
      "Contexto: 'autor' \t Similaridade: 0.9185998833848547\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'campa' \t Similaridade: 0.8662331078831336\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'propriamente' \t Similaridade: 0.8502239740654226\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'berço' \t Similaridade: 0.759624101123346\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'outro' \t Similaridade: 0.7405457025409817\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'segunda' \t Similaridade: 0.732155307316829\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'adotar' \t Similaridade: 0.6639163740090877\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'curo' \t Similaridade: 0.6431057933155517\n",
      "Contexto: 'amei' \t Similaridade: 0.6415948017197486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cálculo dos 10 contextos mais similares a dada palavra utilizando a matriz de Word Embedding\n",
    "def wordsSimilarity(word, matrix, n):\n",
    "    wordIndex = token2index[word]\n",
    "\n",
    "    # Resgate do vetor representante de determinada palavra\n",
    "    if isinstance(matrix, sparse.csr_matrix):\n",
    "        wordVec = matrix.getrow(wordIndex)\n",
    "    else:\n",
    "        wordVec = matrix[wordIndex:wordIndex+1, :]\n",
    "\n",
    "    # Cálculo da similidade (similaridade de vetores por cosseno)\n",
    "    similarity = cosine_similarity(matrix, wordVec).flatten()\n",
    "    sortedIndexes = np.argsort(-similarity)\n",
    "\n",
    "    # Retorno dos n contextos mais similares a dada palavra\n",
    "    similarityContextScores = [(index2token[sortedIndex], similarity[sortedIndex]) \n",
    "                                for sortedIndex in sortedIndexes[0:n]]\n",
    "\n",
    "    return similarityContextScores\n",
    "\n",
    "def wordSimilarityReport(word, matrix, n=10):\n",
    "    similarityContextScores = wordsSimilarity(word, matrix, n)\n",
    "    for context, similarity in similarityContextScores:\n",
    "        print(f'Contexto: \\'{context}\\' \\t Similaridade: {similarity}')\n",
    "\n",
    "        wordParagraphs = [paragraph for paragraph in corpus \n",
    "                          if context in paragraph and word in paragraph][0:5]\n",
    "\n",
    "        for paragraph in wordParagraphs:\n",
    "            print(f'\\\"{paragraph}\\\"', end='\\n\\n')\n",
    "\n",
    "wordSimilarityReport('defunto', wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
