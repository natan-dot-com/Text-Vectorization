{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto Final - Álgebra Linear e Aplicações (SME0142)\n",
    "Projeto confeccionado por Natan Sanches (11795680) e Álvaro José Lopes.\n",
    "\n",
    "- **Objetivo e tematização:** A temática utilizada para confecção do projeto foi a de construção de um _pipeline_ dentro do contexto de Processamento de Linguagem Natural. Um texto bruto pôde ser analisado, permitindo um estudo sobre cada palavra individualmente e sua relação com os contextos em sua vizinhança; técnica usualmente chamada de _Word Embedding_. A partir desse estudo, é possível vetorizar palavras e utilizar técnicas de Álgebra Linear para manipulá-las."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Ferramentas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leitura do texto central\n",
    "Neste exemplo, utilizamos a romance Memórias Póstumas de Brás Cubas (clássico de Machado de Assis). O texto foi carregado a partir de um arquivo em disco, criando um _corpus_ (lista) de parágrafos disponíveis para manipulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"data/BrasCubas-Assis.txt\"\n",
    "with open(inputFile) as f:\n",
    "    corpus = f.readlines()\n",
    "    corpus = [paragraph.rstrip() for paragraph in corpus if paragraph != '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pré-processamento do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em todo projeto envolvendo processamento de língua natural, é necessário que seja feito um pré-processamento que permite consistência na hora da análise textual. Dentre as técnicas de processamento realizadas, estão:\n",
    "\n",
    "- Eliminação de conflitos case-sensitive, isto é, transformando todas as letras maiúsculas em minúsculas.\n",
    "- Eliminação de caracteres não UTF-8, como uma maneira de preservar a consistência do texto.\n",
    "- Eliminação de pontuações e objetos textuais de sinalização, uma vez que estes não serão importantes para a análise.\n",
    "- Eliminação de _stop words_, ou palavras de parada (preposições, conjunções, etc), que não possuem contexto isolado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctranslation = str.maketrans(dict.fromkeys(punctuation))\n",
    "setStopwords = set(stopwords.words(language))\n",
    "\n",
    "# Realiza a tokenização e tratamento dos parágrafos do texto\n",
    "def tokenize(corpus: str) -> list:\n",
    "    corpusTokenized = []\n",
    "    for paragraph in corpus:\n",
    "        paragraph = paragraph.lower()                                       # Tratamento de case-sensitive\n",
    "        paragraph = paragraph.encode('utf8', 'ignore').decode()             # Eliminação de caracteres fora de UTF-8\n",
    "        paragraph = paragraph.translate(punctranslation)                    # Eliminação de pontuações\n",
    "        corpusTokenized.append([token for token in paragraph.split()        # Eliminação de stopwords\n",
    "                                if token not in setStopwords])\n",
    "    return corpusTokenized\n",
    "    \n",
    "corpusTokenized = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Contagem de unigramas do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza, para cada parágrafo, a contagem de frequência de cada _token_ do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "for paragraph in corpusTokenized:\n",
    "    for token in paragraph:\n",
    "        unigrams[token] += 1\n",
    "\n",
    "# Mapeamento de acesso (Token <-> Índice representativo)\n",
    "token2index = {token: index for index, token in enumerate(unigrams.keys())}\n",
    "index2token = {index: token for token, index in token2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contagem de bigramas do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza, para cada parágrafo, a contagem dos bigramas presentes. Um bigrama representa a ocorrência de um par palavra-contexto, de maneira com que o contexto esteja dentro da janela de contexto. Nesse caso, o _range_ da janela de contexto foi de três palavras (para frente e para trás)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contador de bigramas, considerando uma determinada janela de contexto (nesse caso, 2\n",
    "# palavras antes e duas palavras depois)\n",
    "skipgrams = Counter()\n",
    "gap = 3\n",
    "\n",
    "for paragraph in corpusTokenized:\n",
    "    tokens = [token2index[tok] for tok in paragraph]\n",
    "    \n",
    "    # Para cada palavra no parágrafo, realiza a análise dos contextos da vizinhança\n",
    "    for indexWord, word in enumerate(paragraph):\n",
    "        indexContextMin = max(0, indexWord - gap)\n",
    "        indexContextMax = min(len(paragraph)-1, indexWord + gap)\n",
    "\n",
    "        # Para cada contexto da vizinhança, crie um bigrama com a palavra central\n",
    "        indexContexts = [index for index in range(indexContextMin, indexContextMax + 1) if index != indexWord]\n",
    "        for indexContext in indexContexts:\n",
    "            skipgram = (tokens[indexWord], tokens[indexContext])\n",
    "            skipgrams[skipgram] += 1\n",
    "\n",
    "# Exibição dos 10 bigramas mais comuns presentes no texto, considerando a janela de contexto estipulada\n",
    "mostCommon = [(index2token[skipgram[0][0]], index2token[skipgram[0][1]], skipgram[1]) \n",
    "               for skipgram in skipgrams.most_common(10)]\n",
    "mostCommon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Criação da matriz de frequência termo-a-termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3285x3285 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32684 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapeamento das entradas da matriz esparça de frequência entre os bigramas do texto\n",
    "rowsMatrix = []\n",
    "columnsMatrix = []\n",
    "dataMatrix = []\n",
    "\n",
    "for (token1, token2), skipgramCount in skipgrams.items():\n",
    "    rowsMatrix.append(token1)\n",
    "    columnsMatrix.append(token2)\n",
    "    dataMatrix.append(skipgramCount)\n",
    "\n",
    "wwMatrix = sparse.csr_matrix((dataMatrix, (rowsMatrix, columnsMatrix)))\n",
    "wwMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Criação da matriz PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3285x3285 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 32684 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número total de bigramas presente na matriz de frequência\n",
    "# [#(n)]\n",
    "numSkipgrams = wwMatrix.sum()\n",
    "\n",
    "# Mapeamento das entradas da matriz PPMI\n",
    "rowsIndex = []\n",
    "columnsIndex = []\n",
    "ppmiData = []\n",
    "\n",
    "# Vetor de frequência total de cada palavra em todos os possíveis contextos\n",
    "sumWords = np.array(wwMatrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Vetor de frequência total de cada contexto para todas as possíveis palavras\n",
    "sumContexts = np.array(wwMatrix.sum(axis=1)).flatten()\n",
    "\n",
    "for (tokenWord, tokenContext), skipgramCount in skipgrams.items():\n",
    "\n",
    "    # Frequência de determinada palavra em determinado contexto\n",
    "    # [#(w,c)]\n",
    "    freqWordContext = skipgramCount\n",
    "\n",
    "    # Frequência de determinada palavra em todos os contextos possíveis\n",
    "    # [#(w)]\n",
    "    freqWord = sumContexts[tokenWord]\n",
    "    \n",
    "    # Frequência de determinado contexto para todas as palavras possíveis\n",
    "    # [#(c)]\n",
    "    freqContext = sumWords[tokenContext]\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra em determinado contexto\n",
    "    # [P(w,c) = #(w,c) / #(n)]\n",
    "    probWordContext = freqWordContext / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinada palavra individualmente\n",
    "    # [P(w) = #(w) / #(n)]\n",
    "    probWord = freqWord / numSkipgrams\n",
    "\n",
    "    # Probabilidade de ocorrência de determinado contexto individualmente\n",
    "    # [P(c) = #(c) / #(n)]\n",
    "    probContext = freqContext / numSkipgrams\n",
    "\n",
    "    # Cálculo PPMI (Positive Pointwise Mutual Information)\n",
    "    # [PPMI = max(0, log( P(w,c)/(P(w)P(c)) ))]\n",
    "    PPMI = max(np.log2(probWordContext / (probWord * probContext)), 0)\n",
    "\n",
    "    rowsIndex.append(tokenWord)\n",
    "    columnsIndex.append(tokenContext)\n",
    "    ppmiData.append(PPMI)\n",
    "\n",
    "ppmiMatrix = sparse.csr_matrix((ppmiData, (rowsIndex, columnsIndex)))\n",
    "ppmiMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fatoração matricial usando SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds as SVD\n",
    "\n",
    "# Dimensão proposta da matriz de valores singulares produzida pelo SVD\n",
    "# [Hiperparâmetro]\n",
    "embeddingSize = 50\n",
    "\n",
    "U, D, V = SVD(ppmiMatrix, embeddingSize)\n",
    "\n",
    "# Normalização das matrizes de vetores singulares produzidas pelo SVD\n",
    "Unorm = U / np.sqrt(np.sum(U*U, axis=1, keepdims=True))\n",
    "Vnorm = V / np.sqrt(np.sum(V*V, axis=1, keepdims=True))\n",
    "\n",
    "# ???\n",
    "wordVecs = Unorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualização de palavras similares por similaridade por cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: 'defunto'\tSimilaridade: 1.0000000000000004\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "\"-Anda visitando os defuntos? Disse-lhe eu. -Ora, defuntos! respondeu Virgília com um muxoxo. E depois de me apertar as mãos: -Ando a ver se ponho os vadios para a rua.\"\n",
      "\n",
      "\"Logo depois, senti-me transformado na Suma Teologica de São Tomás, impressa num volume, e encadernada em marroquim, com fechos de prata e estampas; idéia esta que me deu ao corpo a mais completa imobilidade; e ainda agora me lembra que, sendo as minhas mãos os fechos do livro, e cruzando-as eu sobre o ventre, alguém as descruzava (Virgília decerto), porque a atitude lhe dava a imagem de um defunto.\"\n",
      "\n",
      "Contexto: 'autor'\tSimilaridade: 0.9345177047311924\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'campa'\tSimilaridade: 0.8797364090356089\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'recente'\tSimilaridade: 0.8619253267207032\n",
      "Contexto: 'propriamente'\tSimilaridade: 0.8334512621898037\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'voltas'\tSimilaridade: 0.8289622201718915\n",
      "Contexto: 'outro'\tSimilaridade: 0.7705162074439288\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'berço'\tSimilaridade: 0.763768701140601\n",
      "\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo; diferença radical entre este livro e o Pentateuco.\"\n",
      "\n",
      "Contexto: 'mil'\tSimilaridade: 0.7378718619216024\n",
      "Contexto: 'improvável'\tSimilaridade: 0.7333624991572129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cálculo dos 10 contextos mais similares a dada palavra utilizando a matriz de Word Embedding\n",
    "def wordsSimilarity(word, matrix, n):\n",
    "    wordIndex = token2index[word]\n",
    "\n",
    "    # Resgate do vetor representante de determinada palavra\n",
    "    if isinstance(matrix, sparse.csr_matrix):\n",
    "        wordVec = matrix.getrow(wordIndex)\n",
    "    else:\n",
    "        wordVec = matrix[wordIndex:wordIndex+1, :]\n",
    "\n",
    "    # Cálculo da similidade (similaridade de vetores por cosseno)\n",
    "    similarity = cosine_similarity(matrix, wordVec).flatten()\n",
    "    sortedIndexes = np.argsort(-similarity)\n",
    "\n",
    "    # Retorno dos n contextos mais similares a dada palavra\n",
    "    similarityContextScores = [(index2token[sortedIndex], similarity[sortedIndex]) \n",
    "                                for sortedIndex in sortedIndexes[:n+1] \n",
    "                                if index2token[sortedIndex] != word]\n",
    "\n",
    "    return similarityContextScores\n",
    "\n",
    "def wordSimilarityReport(word, matrix, n=5):\n",
    "    print(f'\\'{word}\\'\\t Frequência total: {unigrams[word]}', end='\\n\\t')\n",
    "\n",
    "    similarityContextScores = wordsSimilarity(word, matrix, n)\n",
    "    for context, similarity in similarityContextScores:\n",
    "        print(f'Contexto: \\'{context}\\'\\tSimilaridade: {similarity}')\n",
    "\n",
    "        wordParagraphs = [paragraph for paragraph in corpus \n",
    "                          if context in paragraph and word in paragraph][0:5]\n",
    "\n",
    "        for paragraph in wordParagraphs:\n",
    "            print(f'\\\"{paragraph}\\\"', end='\\n\\n')\n",
    "\n",
    "# Obtem os respectivos Word Embeddings\n",
    "embeddings = [wordVecs[token2index[word]] for word in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
